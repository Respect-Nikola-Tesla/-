\section{소감}

\begin{itemize}
    \item[] \textbf{김희윤}: 이번에 정보 엔트로피라는 개념을 처음 접하게 되었다. 정보 이론에서 중요한 개념인 정보 엔트로피의 정의와 그 활용에 대하여 배울 수 있었다. 기존의 거시적이고 확정적인 상황과 다르게 불확실한 상태에 대하여 어떤 식으로 정보를 도출해 내고 계산하는지 알 수 있게 되었다. 유전 알고리즘을 적용 시켜 확률을 계산하는 등 다양한 방법으로 최대 정보 엔트로피를 계산하는 과정을 알게 되었다.
    
    
    \item[] \textbf{남도현}: 정보 엔트로피라는 개념은 이전에도 알고 있었으나 이번 기회를 통해 심도있게 다루고 공부해보아서 유익했다. 특히, kullback-leibler divergence라는 개념으로 확률 분포 사이의 일종의 '거리'를 정의할 수 있다는 점이 신기하였다. 또, 물리에서 맥스웰-볼츠만 분포를 배울 때는 물리적 관점에서 접근하였는데 그 기저에 깔린 가정만으로도 같은 분포를 얻을 수 있다는 점을 알 수 있게 되었고 물리적으로 정의된 2개의 엔트로피와 정보 엔트로피가 일맥 상통한다는 점도 알게 되었다. 여러모로 깨달은 점이 많으며 굉장히 흥미로웠고 앞으로도 볼 일이 자주 있을 것 같다.
    
    \item[] \textbf{배요한}: 이번 프로젝트를 통하여 정보 엔트로피라는 개념을 알게 되었는데, 이를 최대화시키는 과정에서 평균을 설정하거나 표준편차를 설정하는 등의 제약 조건 추가만 하였음에도 우리가 잘 아는 분포가 유도될 수 있다는 것이 흥미로웠다. 또한, 유전 알고리즘을 직접 해보는 과정에서 수렴이 잘 되지 않는 상황이 발생하기도 하였는데, 이 과정에서 선택압을 적절히 조절함으로써 해결될 수 있다는 사실도 알 수 있게 되었다. 결론적으로 작지만 되게 흥미로운 연구였으며 다른 제약 조건이 부여되었을 때의 상황도 궁금해졌다.
    
    
\end{itemize}